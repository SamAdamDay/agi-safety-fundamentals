%!TEX root = ../summaries.tex

\chapter{Learning from humans}

\section{Imitation Learning, Part 1}

\url{https://youtu.be/kGc8jOy5_zY}

\begin{itemize}
    \item Differences when moving from prediction to control for AI systems
    \begin{itemize}
        \item No longer i.i.d.
        \item From ground truth supervision to high-level, abstract goal.
        \item Objective: from predict label to accomplish task.
        \item In real world, some prediction systems also have feedback issues: e.g.\@ traffic prediction system used and affects traffic.
    \end{itemize}
    \item Terminology.
    \begin{itemize}
        \item $\bf o_t$: observation.
        \item $\bf a_t$: action.
        \item $\bf s_t$: state (underlying variables in model).
        \item $\pi_\theta(\bf a_t \mid \bf o_t)$ or $\pi_\theta(\bf a_t \mid \bf s_t)$: policy.
        \item When policy depends on state, it is \emph{fully observed}.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=.95\linewidth]{images/state-bayes-net.png}
            \caption{Bayes' net for the states, observations and actions}
            \label{fig:bayes net states}
        \end{figure}
    \end{itemize}
    \item Initiation learning: supervised learning.
    \begin{itemize}
        \item \emph{Behaviour cloning}.
        \item Doesn't work in theory: small mistakes compound, and we quickly diverge from training distribution.
        \item Works reasonably well in practice, given enough training data.
    \end{itemize}
\end{itemize}


\section{Learning from Human Preferences}

\url{https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/}

\begin{itemize}
    \item Using feedback to infer goal.
    \item Builds model of goal based on feedback.
    \item Learns also when to ask for feedback.
    \item Trained on various games.
    \item Sometimes does better using human feedback than game's actual reward function (the score), since human shapes the reward function better.
    \item Limited by human evaluators intuition on what looks correct.
    \begin{itemize}
        \item On one task, learned to trick human using optical illusion.
    \end{itemize}
\end{itemize}


\section{Learning to Summarize with Human Feedback}

\url{https://openai.com/blog/learning-to-summarize-with-human-feedback/}

\begin{itemize}
    \item Used human feedback to train summarisers.
    \item Produced better performance than much larger models trained only with supervised learning.
    \item Models usually trained with objective to predict next word, but what we really want is high-quality summaries.
    \begin{itemize}
        \item Models can make up things when unsure.
        \item Models can imitate harmful social biases.
    \end{itemize}
    \item First train reward model using supervised learning, then fine-tune language model with reinforcement learning.
    \item Provided frequent feedback to human labellers, to ensure that the labellers were marking according to their goals.
    \item Tested generalisation capacity by using a different dataset, with different types of text.
    \begin{itemize}
        \item Model had been pre-trained on these, but had no human feedback.
        \item Produced high-quality summaries.
        \item When length adjusted, produced even better summaries than the human-made samples.
    \end{itemize}
    \item Core method.
    \begin{enumerate}[label=\arabic*.]
        \item Train initial summariser.
        \item Build dataset of human comparisons between summaries.
        \item Train reward model to predict human preference.
        \item Fine-tune summariser using RL and reward model.
    \end{enumerate}
    \item Took care to ensure high-quality human data.
    \item Optimising reward model eventually lead to quality degradation.
    \begin{itemize}
        \item Reward model only a proxy for human preferences.
        \item Trained on relatively small set of summaries.
        \item When the model optimised to the reward too much, it overfit, and produced poor summaries.
    \end{itemize}
    \item Limitations.
    \begin{itemize}
        \item For more complex tasks, unlikely that researcher labels should be taken as `gold standard'.
        \begin{itemize}
            \item Should hire labellers from impacted groups to define `good' behaviour and reinforce it.
        \end{itemize}
        \item Model trained on Reddit data, and sometimes produced harmful summaries.
        \item Used significant compute resources.
        \item Though outperform human reference summaries, those themselves are not rated very highly on axes of quality (accuracy, coverage, coherence, and overall).
    \end{itemize}
\end{itemize}