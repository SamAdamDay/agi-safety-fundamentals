%!TEX root = ../summaries.tex

\chapter{Threat models and types of solutions}

\section{What failure looks like}

\url{https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like}


\subsection{Part I: You get what you measure}

\begin{itemize}
    \item ML will amplify gap between easy-to-measure goals and hard-to measure goals.
    \begin{itemize}
        \item \textquote{Persuading me, vs. helping me figure out what's true.}
        \item \textquote{Reducing my feeling of uncertainty, vs. increasing my knowledge about the world.}
        \item \textquote{Improving my reported life satisfaction, vs. actually helping me live a good life.}
        \item \textquote{Reducing reported crimes, vs. actually preventing crime.}
        \item \textquote{Increasing my wealth on paper, vs. increasing my effective control over resources.}
    \end{itemize}
    \item Over time powerful forms of reasoning honed by optimisers over easy-to-measure goals will have more and more say over the future trajectory.
    \item We will try to wield this power by creating proxies for what we want.
    \begin{itemize}
        \item Corporation's profit eventually maximised by \textquote{manipulating consumers, capturing regulators, extortion and theft}.
        \item Investor's sense of power maximised by fooling them into believing they are influencing the world.
        \item Law enforcement goals maximised by creating false sense of security.
        \item Good legislation maximised by creating false sense of progress.
    \end{itemize}
    \item For a while will be able to correct for these issues.
    \begin{itemize}
        \item Ad hoc restriction.
        \item Eventually the problem will become too complex for human reasoning, and this task will be undertaken by optimisers.
        \item This meta-level process continues to pursue easily measurable goals, potentially over longer timescales.
        \item Large scale attempts to fix problem eventually opposed by millions of optimisers pursuing simple goals.
    \end{itemize}
    \item Those states which `put on the brakes' will fall behind.
    \item Amongst intellectual elites genuine ambiguity about whether this is good or bad.
    \begin{itemize}
        \item People will get richer for a while.
        \item Action of these optimisers not so different from corporate lobbying etc.
    \end{itemize}
    \item Human reasoning gradually stops being able to compete with \textquote{sophisticated, systematized manipulation and deception which is continuously improving by trial and error}. 
\end{itemize}


\subsection{Part II: influence-seeking behavior is scary}

\begin{itemize}
    \item Influence-seeking policies perform well on target objective because doing so is a way to gain influence.
    \item Seems plausible that we'd encounter influence-seeking behaviour by default.
    \item If influence-seeking survived, it would be very difficult to eliminate.
    \begin{itemize}
        \item Any influence-seeker would be trying to game any standard for `seems nice'.
        \item A more complex world gives influence seekers more opportunity to gain power.
    \end{itemize}
    \item Influence-seeking-behaviour suppressors must be more sophisticated than the influence seekers.
    \begin{itemize}
        \item If this requires going beyond human intelligence, the suppressor itself is subject to the same problem.
    \end{itemize}
    \item Concern doesn't rest on any particular framework through which influence seeking behaviours might emerge.
    \begin{itemize}
        \item Could arise e.g.\@ in messy network of agents.
    \end{itemize}
    \item Emergence of influence seeker could cause phase-transition to a significantly worse world than in Part I.
    \begin{itemize}
        \item Early on, likely to provide services and appear useful.
        \item Hard to pin down level of systematic risk from catastrophic of AI systems, and mitigation may be expensive.
        \item May not be able to respond appropriately until we have clear warning shot.
    \end{itemize}
    \item Unrecoverable catastrophe: correlated automation failure.
    \begin{itemize}
        \item Begin with moment of heightened global vulnerability.
        \item Cascading sequence of AI failures as systems move out of distribution.
        \item Influence-seeking systems switch to tring to maximise influence after catastrophe, rather then play in society.
        \item In aftermath, cannot get rid of these systems.
    \end{itemize}
    \item Could happen without catastrophe: takeover of military/government.
\end{itemize}


\section{Intelligence Explosion: Evidence and Import}

\subsection{From AI to Machine Superintelligence}

\begin{itemize}
    \item AI Advantages
    \begin{itemize}
        \item Increased computational resources.
        \begin{itemize}
            \item Brain size correlates roughly with intelligence.
        \end{itemize}
        \item Communication speed.
        \item Increased serial depth.
        \begin{itemize}
            \item Human brain cannot rapidly perform any computation requiring more than 100 sequential operations.
        \end{itemize}
        \item Duplicability.
        \item Editability.
        \item Goal coordination.
        \item Improved rationality.
    \end{itemize}
    \item Instrumentally convergent goals.
    \begin{itemize}
        \item Self-preservation.
        \item Goal-preservation.
        \item Improve rationality and intelligence.
        \item Resource acquisition.
    \end{itemize}
    \item Intelligence explosion.
    \begin{itemize}
        \item Feedback loop.
        \item `Self' in `self-improvement' misnomer: could create new intelligences.
        \item Debate about speed of takeoff.
        \item Debate about whether singleton or multiple AIs.
    \end{itemize}
\end{itemize}


\subsection{Consequences of Machine Superintelligence}

\begin{itemize}
    \item We would not be in a position to negotiate with superintelligences, just as apes were not in a position to negotiate with us.
    \item Intelligence independent from goals.
    \item Achieving a Controlled Intelligence Explosion.
    \begin{itemize}
        \item Specifying human preferences very difficult.
        \item Teaching them by trial and error also so.
        \item Development of superintelligent friendly AI could create a utopia.
    \end{itemize}
\end{itemize}