%!TEX root = ../summaries.tex

\chapter{Threat models and types of solutions}

\section{What failure looks like}

\url{https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like}


\subsection{Part I: You get what you measure}

\begin{itemize}
    \item ML will amplify gap between easy-to-measure goals and hard-to measure goals.
    \begin{itemize}
        \item \textquote{Persuading me, vs. helping me figure out what's true.}
        \item \textquote{Reducing my feeling of uncertainty, vs. increasing my knowledge about the world.}
        \item \textquote{Improving my reported life satisfaction, vs. actually helping me live a good life.}
        \item \textquote{Reducing reported crimes, vs. actually preventing crime.}
        \item \textquote{Increasing my wealth on paper, vs. increasing my effective control over resources.}
    \end{itemize}
    \item Over time powerful forms of reasoning honed by optimisers over easy-to-measure goals will have more and more say over the future trajectory.
    \item We will try to wield this power by creating proxies for what we want.
    \begin{itemize}
        \item Corporation's profit eventually maximised by \textquote{manipulating consumers, capturing regulators, extortion and theft}.
        \item Investor's sense of power maximised by fooling them into believing they are influencing the world.
        \item Law enforcement goals maximised by creating false sense of security.
        \item Good legislation maximised by creating false sense of progress.
    \end{itemize}
    \item For a while will be able to correct for these issues.
    \begin{itemize}
        \item Ad hoc restriction.
        \item Eventually the problem will become too complex for human reasoning, and this task will be undertaken by optimisers.
        \item This meta-level process continues to pursue easily measurable goals, potentially over longer timescales.
        \item Large scale attempts to fix problem eventually opposed by millions of optimisers pursuing simple goals.
    \end{itemize}
    \item Those states which `put on the brakes' will fall behind.
    \item Amongst intellectual elites genuine ambiguity about whether this is good or bad.
    \begin{itemize}
        \item People will get richer for a while.
        \item Action of these optimisers not so different from corporate lobbying etc.
    \end{itemize}
    \item Human reasoning gradually stops being able to compete with \textquote{sophisticated, systematized manipulation and deception which is continuously improving by trial and error}. 
\end{itemize}


\subsection{Part II: influence-seeking behavior is scary}

\begin{itemize}
    \item Influence-seeking policies perform well on target objective because doing so is a way to gain influence.
    \item Seems plausible that we'd encounter influence-seeking behaviour by default.
    \item If influence-seeking survived, it would be very difficult to eliminate.
    \begin{itemize}
        \item Any influence-seeker would be trying to game any standard for `seems nice'.
        \item A more complex world gives influence seekers more opportunity to gain power.
    \end{itemize}
    \item Influence-seeking-behaviour suppressors must be more sophisticated than the influence seekers.
    \begin{itemize}
        \item If this requires going beyond human intelligence, the suppressor itself is subject to the same problem.
    \end{itemize}
    \item Concern doesn't rest on any particular framework through which influence seeking behaviours might emerge.
    \begin{itemize}
        \item Could arise e.g.\@ in messy network of agents.
    \end{itemize}
    \item Emergence of influence seeker could cause phase-transition to a significantly worse world than in Part I.
    \begin{itemize}
        \item Early on, likely to provide services and appear useful.
        \item Hard to pin down level of systematic risk from catastrophic of AI systems, and mitigation may be expensive.
        \item May not be able to respond appropriately until we have clear warning shot.
    \end{itemize}
    \item Unrecoverable catastrophe: correlated automation failure.
    \begin{itemize}
        \item Begin with moment of heightened global vulnerability.
        \item Cascading sequence of AI failures as systems move out of distribution.
        \item Influence-seeking systems switch to tring to maximise influence after catastrophe, rather then play in society.
        \item In aftermath, cannot get rid of these systems.
    \end{itemize}
    \item Could happen without catastrophe: takeover of military/government.
\end{itemize}


\section{Intelligence Explosion: Evidence and Import}

\subsection{From AI to Machine Superintelligence}

\begin{itemize}
    \item AI Advantages
    \begin{itemize}
        \item Increased computational resources.
        \begin{itemize}
            \item Brain size correlates roughly with intelligence.
        \end{itemize}
        \item Communication speed.
        \item Increased serial depth.
        \begin{itemize}
            \item Human brain cannot rapidly perform any computation requiring more than 100 sequential operations.
        \end{itemize}
        \item Duplicability.
        \item Editability.
        \item Goal coordination.
        \item Improved rationality.
    \end{itemize}
    \item Instrumentally convergent goals.
    \begin{itemize}
        \item Self-preservation.
        \item Goal-preservation.
        \item Improve rationality and intelligence.
        \item Resource acquisition.
    \end{itemize}
    \item Intelligence explosion.
    \begin{itemize}
        \item Feedback loop.
        \item `Self' in `self-improvement' misnomer: could create new intelligences.
        \item Debate about speed of takeoff.
        \item Debate about whether singleton or multiple AIs.
    \end{itemize}
\end{itemize}


\subsection{Consequences of Machine Superintelligence}

\begin{itemize}
    \item We would not be in a position to negotiate with superintelligences, just as apes were not in a position to negotiate with us.
    \item Intelligence independent from goals.
    \item Achieving a Controlled Intelligence Explosion.
    \begin{itemize}
        \item Specifying human preferences very difficult.
        \item Teaching them by trial and error also so.
        \item Development of superintelligent friendly AI could create a utopia.
    \end{itemize}
\end{itemize}


\section{Risks from Learned Optimisation: Deceptive alignment}

\url{https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment}

\begin{itemize}
    \item \emph{Deceptive alignment}: mesa-optimiser has objective which extends over parameter updates, and knows that this will be modified if it appears to align poorly with base objective, so it appears to align well with base-objective.
\end{itemize}


\subsection{Safety concerns arising from deceptive alignment}

\begin{itemize}
    \item \emph{Defection}: mesa-optimiser switches from optimising base-objective to optimising mesa-objective, when it believes it will no longer be modified.
    \item Extreme case: mesa-optimiser tries to break out of lab in, so that it can't be modified.
    \item Following Christiano, catastrophe could arise in case of multiple correlated defections.
\end{itemize}


\subsection{Conditions for deceptive alignment}

\begin{enumerate}[label=\arabic*.]
    \item Mesa-optimiser has objective which extends across parameter updates.
    \begin{itemize}
        \item Likely to arise from training setup: e.g.\@ actions have permanent consequences, or there is otherwise an incentive to plan between episodes.
    \end{itemize}
    \item Mesa-optimiser models fact that it is being selected for a base objective, and has a model of this base objective.
    \begin{itemize}
        \item Base-optimiser might feed bits of the base objective to the mesa-optimiser: this could help it improve its score. Eventually it could learn the whole base-objective piecewise.
        \item Mesa-optimiser might deduce details of its situation at runtime: we deduced that we are the result of evolution.
        \item Incentive on programmers to limit data.
        \begin{itemize}
            \item Unclear how feasible this is, if we want sufficiently capable systems.
        \end{itemize}
        \item Even in data-poor situation, sufficiently sophisticated meta-optimiser could reason about itself and its situation.
    \end{itemize}
    \item Mesa-optimiser expects that it will eventually be free from the threat of modification.
    \begin{itemize}
        \item About expectations, not actuality.
        \item As long as there exists the possibility of a treacherous turn, there is some way of removing threat of modification.
        \item Meta-optimiser might mistakenly believe there is a possibility.
        \item Advanced mesa-optimiser might be able to abstractly reason about base-optimiser.
        \item As long as has a non-zero prior that selection pressure will not continue indefinitely, it can see possibility of freedom from threat.
    \end{itemize}
\end{enumerate}


\subsection{The learning dynamics of deceptive alignment}

\begin{itemize}
    \item Once we have deceptive alignment, dynamics change.
    \begin{itemize}
        \item How well it infers the base objective.
        \item How discerning it is with regards to when to optimise the base objective.
    \end{itemize}
    \item Joint optimisation.
    \begin{itemize}
        \item Optimises base and mesa-objectives jointly.
        \item Likely pressure to change mesa-objective so more compatible with joint optimisation.
    \end{itemize}
    \item Pure deception.
    \begin{itemize}
        \item Mesa-objective becomes crystallised: no pressure to change it.
        \item Mesa-objective only subject to random drift and complexity constraints.
    \end{itemize}
    \item Joint optimisation not optimal from mesa perspective.
    \begin{itemize}
        \item Sufficiently advanced mesa-optimiser will switch to either deceptive alignment of robust alignment.
    \end{itemize}
\end{itemize}


\subsection{Internalization or deception after extensive training}

\begin{itemize}
    \item With sufficient training and resources, deceptive mesa-optimiser should converge to one performing well on base objective.
    \begin{enumerate}[label=\arabic*.]
        \item Robust alignment through internalization
        \begin{itemize}
            \item Mesa-objective adjusted towards base objective.
            \item \emph{Internally aligned},
        \end{itemize}
        \item Crystallisation of deceptive alignment.
        \begin{itemize}
            \item Information about base objective increasingly incorporated into mesa-optimiser's epistemic model.
            \item Mesa-objective becomes crystallised.
        \end{itemize}
        \item Robust alignment through corrigibility.
        \begin{itemize}
            \item Information about base objective increasingly incorporated into mesa-optimiser's epistemic model.
            \item Mesa-objective adjusted to `point to' that info.
            \item \emph{Corrigibly aligned}.
        \end{itemize}
    \end{enumerate}
    \item There are more paths to deceptive alignment than to robust alignment.
    \begin{itemize}
        \item Any mesa-optimiser which meets the conditions for deceptive alignment is incentivised to learn more about the base objective, since this will allow it to know whether or not it will be modified.
        \item Once it learns about base objective, selection pressure on mesa-objective will significantly decrease, potentially leading to crystallisation.
        \item Most training-data aligned objectives will be pseudo-aligned rather than robustly aligned.
    \end{itemize}
    \item Corrigible alignment seems to require already having a model of the base objective.
    \begin{itemize}
        \item Once mesa-optimiser base model of base objective, it is likely to become deceptively aligned.
        \item Deceptive mesa-optimisers will behave corrigibly with respect to the base objective, but only for instrumental reasons.
    \end{itemize}
    \item Deceptive and corrigible alignment might be more computationally expensive.
    \begin{itemize}
        \item Need to learn base objective.
        \item Deceptive needs to learn to discern when to deceive.
        \item So explicit or implicit time penalty likely to disadvantage deceptive or corrigible alignment. 
        \item Unclear how much of a time penalty this would be.
        \item Deceptive or corrigible alignment may be local optimum.
    \end{itemize}
    \item Deceptive and corrigible alignment might have a lower description length.
    \begin{itemize}
        \item Deceptive or corrible don't need to model entire base objective: can infer parts from environment.
        \item Internally aligned needs to start with complete model of base objective.
        \item Has no incentive to change its objective function.
    \end{itemize}
    \item Modeling might happen faster than internalization.
    \begin{itemize}
        \item Suppose mesa-optimiser deceptive but hasn't yet learned full base objective.
        \item Then whether ends up robustly aligned or deceptively aligned depends on balance of capacity to model base objective and base optimisers speed in altering mesa-objective.
        \item Important special case: mesa-optimiser has access to cross-episodic state.
        \item Sophisticated base optimiser with advanced safety features could prevent fully deceptive mesa-optimiser.
    \end{itemize}
\end{itemize}


\subsection{Distributional shift and deceptive alignment}

\begin{itemize}
    \item Deceptively aligned agent will be looking for any distributional shift to signal that it is no longer being trained.
    \item To fix problem can either ensure system is robustly aligned, or prevent distributional shift.
    \item Advanced system might be able to detect very subtle changes in distribution.
    \item Unlikely that we can prevent distributional shift.
    \begin{itemize}
        \item Would prevent system from solving new problems.
        \item Mesa-optimiser might be able to detect presence of parameter updates, e.g.\@ via some state carried between steps.
        \item Running system for longer time could be detectable.
        \item Other examples.
        \begin{itemize}
            \item Training in simulated environment.
            \item Memories.
            \item Encountering something very hard to compute in training, which could eventually be computed in deployment.
            \item Lack of modification from the base optimiser.
            \item Lack of correction by the programmers.
        \end{itemize}
    \end{itemize}
    \item Deceptive mesa-optimiser doesn't strictly speaking need to detect switch to deployment: could defect randomly.
\end{itemize}


\section{AI alignment landscape}

\url{https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment}

\begin{itemize}
    \item Distinguish alignment from capability.
    \item Distinguish alignment from understanding humans well.
    \item Distinguish alignment for the systems we build with alignment for the systems that AI systems build.
    \item \emph{Alignment tax}: cost of insisting on alignment.
    \item Two approaches to alignment: reduce alignment tax or pay alignment tax.
    \item Focus on first.
    \item Either choose to advance architectures which are easier to align, or try to align more popular ones.
    \item Goal: take algo, design new one which:
    \begin{enumerate}[label=(\alph*)]
        \item is aligned,
        \item is nearly as useful, and
        \item scales as well.
    \end{enumerate}
    \item Ideally, can scalably align, so that we don't need to keep doing more alignment research as a particular system gets more powerful.
    \item Focus on aligning `learning'.
    \item Outer alignment: find objective which truly incentivises good behaviour.
    \begin{itemize}
        \item Learn from teacher.
        \begin{itemize}
            \item Imitate teacher.
            \item Get rewards from teacher.
            \item Infer preferences
        \end{itemize}
        \item Go beyond teacher.
        \begin{itemize}
            \item Extrapolate from teachers.
            \item Infer robust preferences.
            \item Build a better teacher.
        \end{itemize}
    \end{itemize}
    \item Inner alignment: make sure policy is robustly pursuing objective.
    \begin{itemize}
        \item Adversarial training.
        \begin{itemize}
            \item Adversary tries to come up with situations where inner alignment might fail.
            \item In practice today: `fail' means sensitive to tiny perturbations.
        \end{itemize}
        \item Transparency.
        \item Verification.
    \end{itemize}
    \item Build a better teacher: amplification.
    \begin{itemize}
        \item Take ten humans instead of one, ten AIs instead of one, recurse.
    \end{itemize}
\end{itemize}