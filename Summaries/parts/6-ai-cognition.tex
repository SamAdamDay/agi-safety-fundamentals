%!TEX root = ../summaries.tex

\chapter{Towards a principled understanding of AI cognition}

\section{Feature Visualization}

\url{https://distill.pub/2017/feature-visualization/}

\begin{itemize}
    \item Two main threads of interpretability research: feature visualisation and attribution.
    \item Feature Visualization by Optimization.
    \begin{itemize}
        \item Exploits the fact that fact that neural networks are differentiable to find out what input would cause a certain behaviour.
        \item Iteratively tweaks input.
        \item Optimization Objectives.
        \begin{itemize}
            \item To understand individual features: search for examples that give high values either for a neuron or a whole channel.
            \item To understand layer: use DeepDream objective, looking for inputs the layer finds `interesting'.
            \item To understand classifier: search for examples optimising either class logits (before softmax) or class probabilities (after softmax).
            \begin{itemize}
                \item Experience: easiest way to maximise probability is to make everything else very unlikely.
                \item So optimising logits produces better examples.
            \end{itemize}
            \item Many other objectives possible.
            \begin{itemize}
                \item Objectives used in style transfer.
                \item Objectives used in optimisation-based model inversion.
            \end{itemize}
        \end{itemize}
        \item Why visualise by optimisation?
        \begin{itemize}
            \item Why not look through dataset for examples?
            \item Visualising by optimisation allows us to see what the network is really looking for.
            \begin{itemize}
                \item Separates things causing behaviour from things which merely correlate.
                \item Might think (from dataset) that model is looking for buildings, but it's really looking for sky.
            \end{itemize}
            \item Flexibility.
            \begin{itemize}
                \item E.g.\@ want to know how neurons jointly represent info, so can ask what needs to be changed in input to get other ones to fire.
                \item Allows us to visualise how model evolves as it trains.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item Diversity.
    \begin{itemize}
        \item Examples capture full range of feature?
        \item Dataset examples have advantage: capture whole range of ways that neuron can be activated.
        \item Optimisation generally gives just one really positive example (and maybe one really negative one).
        \item Achieving Diversity with Optimisation.
        \begin{itemize}
            \item A classier may be trained to classify a range of different inputs under one class.
            \item Wei et al.\@ attempt to show this diversity using clustering.
            \item Nguyen, Yosinski, and collaborators search through dataset for diverse examples and use these as starting points for optimisation.
            \item In later work, combine visualising classes with generative model.
            \item Simple method: add `diversity' term to objective, to force examples to be different.
            \begin{itemize}
                \item Downside: forcing diversity can cause unnatural artifacts, or examples to differ in strange ways.
            \end{itemize}
            \item Generating diverse examples enables us to get a fuller picture of what causes the neuron to activate.
        \end{itemize}
        \item Neurons don't necessarily correspond to single concepts: can represent strange mixtures of ideas.
        \begin{itemize}
            \item Neuron not necessarily right semantic unit for understanding neural nets.
        \end{itemize}
    \end{itemize}
    \item Interaction between Neurons.
    \begin{itemize}
        \item Think about neuron activation space.
        \item Individual neurons give basis vectors.
        \item Random directions can be just as meaningful as basis directions.
        \item But basis vector directions interpretable more often than random directions.
        \item Summing neurons which `mean' different things can produce interesting combinations.
        \item Can also interpolate.
        \item Don't know how to select meaningful directions.
        \item Don't know how to investigate how large number of directions interact (interpolation only works for a small number).
    \end{itemize}
    \item The Enemy of Feature Visualization.
    \begin{itemize}
        \item Optimising leads to an image full of high-frequency patterns and noise.
        \item Seems related to phenomenon of adversarial examples.
        \item Important part of the reason for this seems to be the use of strided convolution layers and pooling operations.
        \item The Spectrum of Regularization.
        \begin{itemize}
            \item In order to get useful visualisations, need to impose regularisation, noise or constraint.
            \item Spectrum of regularisation, from weak to strong.
            \begin{enumerate}[label=(\roman*)]
                \item Unregularised.
                \item Frequency penalisation.
                \item Transformation robustness.
                \item Learned prior.
                \item Dataset examples.
            \end{enumerate}
        \end{itemize}
        \item Frequency penalization.
        \begin{itemize}
            \item Directly targets high frequency noise.
            \item Either explicitly penalises high variation between neighbouring pixles, or implicitly using blur.
            \item Discourage legitimate high-frequency features like edges.
            \begin{itemize}
                \item Can be slightly improved using bilateral filter, which preserves edges.
            \end{itemize}
        \end{itemize}
        \item Transformation robustness.
        \begin{itemize}
            \item Tries to find examples that are robust to transformation.
            \item For images even a small amount seems to be effective.
            \item Stochastically jitter, rotate and scale before stepping the optimiser.
        \end{itemize}
        \item Learned priors.
        \begin{itemize}
            \item Try to learn a model of the real data and use that as prior.
            \item Can generate photorealistic images.
            \item Can be hard to tell what came from the model being visualised and what from the prior.
            \item On approach: optimise within latent space given by generative model.
            \item Another: jointly optimise prior along with objective.
        \end{itemize}
    \end{itemize}
    \item Preconditioning and Parameterisation.
    \begin{itemize}
        \item About techniques not really reguliser on output example, but transformation of gradient.
        \item This is preconditioning: essentially a reparametrisation which changes the optimisation landscape, preserving the minima but changing the steepness of points.
        \begin{itemize}
            \item Powerful technique in optimisation.
            \item Can make optimisation problem much easier.
        \end{itemize}
        \item Good first guess at what preconditioner to use is one which makes data decorrelated.
        \begin{itemize}
            \item For images means doing gradient descent in Fourier basis with frequencies scaled so they have equal energy.
            \item Resulting visualisations seem a lot better.
        \end{itemize}
    \end{itemize}
\end{itemize}


\section{Zoom In: An Introduction to Circuits}

\url{https://distill.pub/2020/circuits/zoom-in/}

\begin{itemize}
    \item \emph{Zooming in}: period of scientific advancement in which increase in precision leads to qualitative change in capacity to investigate.
    \item Neural networks have a rich inner world.
    \item Examine networks at scale of individual neurons or even individual weights.
\end{itemize}


\subsection{Three Speculative Claims}

\begin{enumerate}[label={\bfseries Claim \arabic*.}, leftmargin=*, labelindent=3pt]
    \item Features.
    \begin{itemize}[leftmargin=-10pt]
        \item Features fundamental unit of neural networks.
        \item Correspond to directions in neuron activation space.
        \item Can be rigorously studied and understood.
    \end{itemize}
    \item Circuits.
    \begin{itemize}[leftmargin=-10pt]
        \item Features come together as circuits, connected by weights.
        \item Can be rigorously studied and understood.
    \end{itemize}
    \item Universality.
    \begin{itemize}[leftmargin=-10pt]
        \item Features and circuits exist across all models.
    \end{itemize}
\end{enumerate}


\subsection{Claim 1: Features}

\begin{itemize}
    \item Later layers contain higher level features.
    \item Community divided on existence of features.
    \item Authors believe, after 1000s hours studying individual neurons that neurons typically are understandable.
    \item Understandable $\neq$ simple.
    \item But typically eventually see neuron as doing something quite natural.
    \item Example 1: Curve Detectors.
    \begin{itemize}
        \item Curve-detecting neurons found in every nontrivial vision model examined.
        \item Straddle boundary between features generally agreed to exist (edges) and features about which there is scepticism.
        \item Authors believe evidence is strong that these are really detecting curves.
        \begin{enumerate}[label={\bfseries Argument \arabic*.}, leftmargin=*, labelindent=3pt]
            \item Feature visualisation
            \begin{itemize}[leftmargin=-30pt]
                \item Optimising to get the neurons to fire reliably produces curves.
                \item Establishes causal link: everything example added to cause neuron to fire.
            \end{itemize}
            \item Dataset examples
            \begin{itemize}[leftmargin=-30pt]
                \item Dataset examples which cause neurons to fire are reliably curves.
                \item Moderate firing examples are generally less-perfect curves.
            \end{itemize}
            \item Synthetic examples.
            \begin{itemize}[leftmargin=-30pt]
                \item Fire for range of synthetic examples of curves.
                \item Only fire near expected orientation, and don't fire for straight lines or corners.
            \end{itemize}
            \item Joint tuning.
            \begin{itemize}[leftmargin=-30pt]
                \item Firing changes continuously as examples rotated, and other neurons for other orientations start firing as the first stops.
            \end{itemize}
            \item Feature implementation.
            \begin{itemize}[leftmargin=-30pt]
                \item By looking at circuit constructing curve detector, can read off algo for detecting curves.
                \item Can't see suggestion of alternative reading, though some small weights not understood.
            \end{itemize}
            \item Feature use.
            \begin{itemize}[leftmargin=-30pt]
                \item Downstream users of neurons use curves in expected way and detect things involving curves.
            \end{itemize}
            \item Handwritten circuits.
            \begin{itemize}[leftmargin=-30pt]
                \item Can reimplement curve detector by manually specifying the weights.
            \end{itemize}
        \end{enumerate}
        \item Claims don't fully eliminate possibility of secondary interpretation, but these activations are likely much weaker.
        \item Meets evidentiary used in neuroscience.
    \end{itemize}
    \item Example 2: High-Low Frequency Detectors.
    \begin{itemize}
        \item Example of feature which is less intuitive, but once understood seems natural and elegant.
        \item Look for high-frequency patterns on one side of the receptive field, and low-frequency patterns on the other.
        \item Appear to be one of the heuristics used for detecting boundaries.
        \item Arguments above can be tweaked to provide strong case that these features exist.
    \end{itemize}
    \item Example 3: Pose-Invariant Dog Head Detector.
    \begin{itemize}
        \item Higher-level feature.
        \item Can also adapt arguments above.
        \begin{itemize}
            \item For example, can use 3D dog head to generate synthetic examples.
            \item Some arguments require quite a bit of effort. Circuit arguments (below) more scalable.
        \end{itemize}
    \end{itemize}
    \item Polysemantic Neurons.
    \begin{itemize}
        \item Some neurons respond to a whole range of types of features.
        \item Not responding to some commonality between features.
        \item Present challenge for circuits agenda.
        \item Hope that polysemantic neurons can be resolved.
        \begin{itemize}
            \item By `unfolding network' to convert them into pure features.
            \item Training networks not to exhibit them.
            \item Essentially problem studied in literature on disentangling representations.
        \end{itemize}
        \item Why do they form?
        \begin{itemize}
            \item Superposition (below).
        \end{itemize}
    \end{itemize}
\end{itemize}