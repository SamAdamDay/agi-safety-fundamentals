%!TEX root = ../summaries.tex

\chapter{Towards a principled understanding of AI cognition}

\section{Feature Visualization}

\url{https://distill.pub/2017/feature-visualization/}

\begin{itemize}
    \item Two main threads of interpretability research: feature visualisation and attribution.
    \item Feature Visualization by Optimization.
    \begin{itemize}
        \item Exploits the fact that fact that neural networks are differentiable to find out what input would cause a certain behaviour.
        \item Iteratively tweaks input.
        \item Optimization Objectives.
        \begin{itemize}
            \item To understand individual features: search for examples that give high values either for a neuron or a whole channel.
            \item To understand layer: use DeepDream objective, looking for inputs the layer finds `interesting'.
            \item To understand classifier: search for examples optimising either class logits (before softmax) or class probabilities (after softmax).
            \begin{itemize}
                \item Experience: easiest way to maximise probability is to make everything else very unlikely.
                \item So optimising logits produces better examples.
            \end{itemize}
            \item Many other objectives possible.
            \begin{itemize}
                \item Objectives used in style transfer.
                \item Objectives used in optimisation-based model inversion.
            \end{itemize}
        \end{itemize}
        \item Why visualise by optimisation?
        \begin{itemize}
            \item Why not look through dataset for examples?
            \item Visualising by optimisation allows us to see what the network is really looking for.
            \begin{itemize}
                \item Separates things causing behaviour from things which merely correlate.
                \item Might think (from dataset) that model is looking for buildings, but it's really looking for sky.
            \end{itemize}
            \item Flexibility.
            \begin{itemize}
                \item E.g.\@ want to know how neurons jointly represent info, so can ask what needs to be changed in input to get other ones to fire.
                \item Allows us to visualise how model evolves as it trains.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item Diversity.
    \begin{itemize}
        \item Examples capture full range of feature?
        \item Dataset examples have advantage: capture whole range of ways that neuron can be activated.
        \item Optimisation generally gives just one really positive example (and maybe one really negative one).
        \item Achieving Diversity with Optimisation.
        \begin{itemize}
            \item A classier may be trained to classify a range of different inputs under one class.
            \item Wei et al.\@ attempt to show this diversity using clustering.
            \item Nguyen, Yosinski, and collaborators search through dataset for diverse examples and use these as starting points for optimisation.
            \item In later work, combine visualising classes with generative model.
            \item Simple method: add `diversity' term to objective, to force examples to be different.
            \begin{itemize}
                \item Downside: forcing diversity can cause unnatural artifacts, or examples to differ in strange ways.
            \end{itemize}
            \item Generating diverse examples enables us to get a fuller picture of what causes the neuron to activate.
        \end{itemize}
        \item Neurons don't necessarily correspond to single concepts: can represent strange mixtures of ideas.
        \begin{itemize}
            \item Neuron not necessarily right semantic unit for understanding neural nets.
        \end{itemize}
    \end{itemize}
    \item Interaction between Neurons.
    \begin{itemize}
        \item Think about neuron activation space.
        \item Individual neurons give basis vectors.
        \item Random directions can be just as meaningful as basis directions.
        \item But basis vector directions interpretable more often than random directions.
        \item Summing neurons which `mean' different things can produce interesting combinations.
        \item Can also interpolate.
        \item Don't know how to select meaningful directions.
        \item Don't know how to investigate how large number of directions interact (interpolation only works for a small number).
    \end{itemize}
    \item The Enemy of Feature Visualization.
    \begin{itemize}
        \item Optimising leads to an image full of high-frequency patterns and noise.
        \item Seems related to phenomenon of adversarial examples.
        \item Important part of the reason for this seems to be the use of strided convolution layers and pooling operations.
        \item The Spectrum of Regularization.
        \begin{itemize}
            \item In order to get useful visualisations, need to impose regularisation, noise or constraint.
            \item Spectrum of regularisation, from weak to strong.
            \begin{enumerate}[label=(\roman*)]
                \item Unregularised.
                \item Frequency penalisation.
                \item Transformation robustness.
                \item Learned prior.
                \item Dataset examples.
            \end{enumerate}
        \end{itemize}
        \item Frequency penalization.
        \begin{itemize}
            \item Directly targets high frequency noise.
            \item Either explicitly penalises high variation between neighbouring pixles, or implicitly using blur.
            \item Discourage legitimate high-frequency features like edges.
            \begin{itemize}
                \item Can be slightly improved using bilateral filter, which preserves edges.
            \end{itemize}
        \end{itemize}
        \item Transformation robustness.
        \begin{itemize}
            \item Tries to find examples that are robust to transformation.
            \item For images even a small amount seems to be effective.
            \item Stochastically jitter, rotate and scale before stepping the optimiser.
        \end{itemize}
        \item Learned priors.
        \begin{itemize}
            \item Try to learn a model of the real data and use that as prior.
            \item Can generate photorealistic images.
            \item Can be hard to tell what came from the model being visualised and what from the prior.
            \item On approach: optimise within latent space given by generative model.
            \item Another: jointly optimise prior along with objective.
        \end{itemize}
    \end{itemize}
    \item Preconditioning and Parameterisation.
    \begin{itemize}
        \item About techniques not really reguliser on output example, but transformation of gradient.
        \item This is preconditioning: essentially a reparametrisation which changes the optimisation landscape, preserving the minima but changing the steepness of points.
        \begin{itemize}
            \item Powerful technique in optimisation.
            \item Can make optimisation problem much easier.
        \end{itemize}
        \item Good first guess at what preconditioner to use is one which makes data decorrelated.
        \begin{itemize}
            \item For images means doing gradient descent in Fourier basis with frequencies scaled so they have equal energy.
            \item Resulting visualisations seem a lot better.
        \end{itemize}
    \end{itemize}
\end{itemize}


\section{Zoom In: An Introduction to Circuits}

\url{https://distill.pub/2020/circuits/zoom-in/}

\begin{itemize}
    \item \emph{Zooming in}: period of scientific advancement in which increase in precision leads to qualitative change in capacity to investigate.
    \item Neural networks have a rich inner world.
    \item Examine networks at scale of individual neurons or even individual weights.
\end{itemize}


\subsection{Three Speculative Claims}

\begin{enumerate}[label={\bfseries Claim \arabic*.}, leftmargin=*, labelindent=3pt]
    \item Features.
    \begin{itemize}[leftmargin=-10pt]
        \item Features fundamental unit of neural networks.
        \item Correspond to directions in neuron activation space.
        \item Can be rigorously studied and understood.
    \end{itemize}
    \item Circuits.
    \begin{itemize}[leftmargin=-10pt]
        \item Features come together as circuits, connected by weights.
        \item Can be rigorously studied and understood.
    \end{itemize}
    \item Universality.
    \begin{itemize}[leftmargin=-10pt]
        \item Features and circuits exist across all models.
    \end{itemize}
\end{enumerate}


\subsection{Claim 1: Features}

\begin{itemize}
    \item Later layers contain higher level features.
    \item Community divided on existence of features.
    \item Authors believe, after 1000s hours studying individual neurons that neurons typically are understandable.
    \item Understandable $\neq$ simple.
    \item But typically eventually see neuron as doing something quite natural.
    \item Example 1: Curve Detectors.
    \begin{itemize}
        \item Curve-detecting neurons found in every nontrivial vision model examined.
        \item Straddle boundary between features generally agreed to exist (edges) and features about which there is scepticism.
        \item Authors believe evidence is strong that these are really detecting curves.
        \begin{enumerate}[label={\bfseries Argument \arabic*.}, leftmargin=*, labelindent=3pt]
            \item Feature visualisation
            \begin{itemize}[leftmargin=-30pt]
                \item Optimising to get the neurons to fire reliably produces curves.
                \item Establishes causal link: everything example added to cause neuron to fire.
            \end{itemize}
            \item Dataset examples
            \begin{itemize}[leftmargin=-30pt]
                \item Dataset examples which cause neurons to fire are reliably curves.
                \item Moderate firing examples are generally less-perfect curves.
            \end{itemize}
            \item Synthetic examples.
            \begin{itemize}[leftmargin=-30pt]
                \item Fire for range of synthetic examples of curves.
                \item Only fire near expected orientation, and don't fire for straight lines or corners.
            \end{itemize}
            \item Joint tuning.
            \begin{itemize}[leftmargin=-30pt]
                \item Firing changes continuously as examples rotated, and other neurons for other orientations start firing as the first stops.
            \end{itemize}
            \item Feature implementation.
            \begin{itemize}[leftmargin=-30pt]
                \item By looking at circuit constructing curve detector, can read off algo for detecting curves.
                \item Can't see suggestion of alternative reading, though some small weights not understood.
            \end{itemize}
            \item Feature use.
            \begin{itemize}[leftmargin=-30pt]
                \item Downstream users of neurons use curves in expected way and detect things involving curves.
            \end{itemize}
            \item Handwritten circuits.
            \begin{itemize}[leftmargin=-30pt]
                \item Can reimplement curve detector by manually specifying the weights.
            \end{itemize}
        \end{enumerate}
        \item Claims don't fully eliminate possibility of secondary interpretation, but these activations are likely much weaker.
        \item Meets evidentiary used in neuroscience.
    \end{itemize}
    \item Example 2: High-Low Frequency Detectors.
    \begin{itemize}
        \item Example of feature which is less intuitive, but once understood seems natural and elegant.
        \item Look for high-frequency patterns on one side of the receptive field, and low-frequency patterns on the other.
        \item Appear to be one of the heuristics used for detecting boundaries.
        \item Arguments above can be tweaked to provide strong case that these features exist.
    \end{itemize}
    \item Example 3: Pose-Invariant Dog Head Detector.
    \begin{itemize}
        \item Higher-level feature.
        \item Can also adapt arguments above.
        \begin{itemize}
            \item For example, can use 3D dog head to generate synthetic examples.
            \item Some arguments require quite a bit of effort. Circuit arguments (below) more scalable.
        \end{itemize}
    \end{itemize}
    \item Polysemantic Neurons.
    \begin{itemize}
        \item Some neurons respond to a whole range of types of features.
        \item Not responding to some commonality between features.
        \item Present challenge for circuits agenda.
        \item Hope that polysemantic neurons can be resolved.
        \begin{itemize}
            \item By `unfolding network' to convert them into pure features.
            \item Training networks not to exhibit them.
            \item Essentially problem studied in literature on disentangling representations.
        \end{itemize}
        \item Why do they form?
        \begin{itemize}
            \item Superposition (below).
        \end{itemize}
    \end{itemize}
\end{itemize}


\subsection{Claim 2: Circuits}

\begin{itemize}
    \item Study relationships between layers by studying circuits: subgraphs consisting of tightly linked features and weights between them.
    \item Circuits surprisingly tractable an meaningful.
    \item Can read off algorithms from circuits.
    \item Circuit 1: Curve Detectors.
    \begin{itemize}
        \item Primarily implemented using earlier, more primitive curve detectors, and edge detectors.
        \item Long tail of other features which make minor contribution.
        \item Focus on relationship between primitive curve detectors and later ones.
        \item Given a particular primitive one, later ones of the same orientation have curve of positive weights through convolution for the primitive one.
        \item Later detector looking for `tangent curve' using earlier detectors.
        \item True of all orientations.
        \item Earlier curve detectors of the wrong orientation inhibited in later detectors.
        \item Gets richer closer you look.
        \item Weights rotate with orientation of curve: \emph{equivariant circuit}.
    \end{itemize}
    \item Circuit 2: Oriented Dog Head Detection.
    \begin{itemize}
        \item Spans 4 layers.
        \item Implements sophisticated equivariance.
        \item ImageNet model must develop many neurons for dog recognition.
        \item Circuit for recognising dogs facing left and dogs facing right.
        \begin{itemize}
            \item Two mirrored pathways over 3 layers.
            \item At each stage, inhibit each other, sharpening contrast.
            \item Invariant neurons at end which respond to both pathways.
            \item Pattern called \emph{unioning over cases}.
            \begin{itemize}
                \item Separately detects different cases, then takes union to make invariant, multifaceted units.
                \item Mutual inhibition: behaves like XOR.
            \end{itemize}
            \item Surprising that network has learned to do this: could have done something much less sophisticated.
            \item Looking at convolutions: head only detected on correct side.
        \end{itemize}
    \end{itemize}
    \item Circuit 3: Cars in Superposition.
    \begin{itemize}
        \item Car detecting neuron.
        \item Looks for windows on bottom of convolution window, and wheels on bottom.
        \item Then spreads car features over neurons which seem to be primarily doing something else.
        \item Looks deliberate.
        \item \emph{Superposition}.
        \item Authors believe it allows network to conserve neurons for other more important things.
        \item If two things don't co-occur (e.g.\@ cars and dogs), they can be dealt with by one higher-level neuron.
    \end{itemize}
    \item Circuit Motifs.
    \begin{itemize}
        \item Same motifs reappear throughout circuits.
        \begin{itemize}
            \item Equivariance.
            \item Unioning over cases.
            \item Superposition.
        \end{itemize}
        \item Understanding motifs likely more important than understanding individual circuits, in the long run.
    \end{itemize}
\end{itemize}


\subsection{Claim 3: Universality}

\begin{itemize}
    \item Widely accepted that first layer of vision networks form Gabor layers.
    \item If meaningful features form in latter layers, perhaps not surprising if same features form in layers beyond the first.
    \item Also perhaps not surprising if features combine in the same way.
    \item Prior work: different networks develop highly correlated neurons, and similar representations in hidden layers.
    \begin{itemize}
        \item But high correlation doesn't necessarily imply similar features.
    \end{itemize}
    \item Ideally: like to determine features, and show that \emph{these} are universal across models.
    \item Similarly for circuits.
    \item Only anecdotal evidence so far: similar low-level features form in many different network architectures, and with two different datasets.
    \item If universality true, might we also hope to find similar features in biological networks?
    \begin{itemize}
        \item Some work has shown that neural networks can be helpful in analysing biological systems.
    \end{itemize}
    \item If universality false, not fatal, but influences what kind of research makes sense.
    \begin{itemize}
        \item Would need to focus on a subset of models with particular societal importance.
    \end{itemize}
\end{itemize}


\subsection{Interpretability as a Natural Science}

\begin{itemize}
    \item Thomas Kuhn distinguishes `normal science', in which researchers have common paradigm, with `extraordinary science', in which there is none.
    \item Extraordinary science not good place to be.
    \item Looks like interpretability is an extraordinary science/pre-paradigmatic right now.
    \item There isn't a common framework for evaluating work.
    \begin{itemize}
        \item Some researchers want `interpretability benchmark'.
        \item Others want it to be based on user studies.
    \end{itemize}
    \item Third proposal: interpretability is empirical: we take neural networks as object of study.
    \item Neural nets very complicated objects: hard to formalise interesting empirical statements.
    \item Circuits sidestep this issue.
    \begin{itemize}
        \item Falsifiable.
        \item Small enough circuits could be investigated rigorously/mathematically.
    \end{itemize}
    \item Circuits could act as epistemic foundation for interpretability.
\end{itemize}


\section{Chris Olah's views on AGI safety}

\url{https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety}


\subsection{The benefits of transparency and interpretability}

\begin{itemize}
    \item Catching problems with auditing.
    \begin{itemize}
        \item Give tools to audit AI before deployment.
        \item If it's not learned the true objective, but a proxy, can throw it out.
        \item Helps with a lot of robustness problems, provided that the AI is not trying to actively deceive tools.
        \item Ideally, should be an ongoing process throughout development/training.
        \item One of OpenAI Clarity team major research direction: more systematic auditing.
        \begin{itemize}
            \item Want interpretability tools to reliably be able to catch problems.
            \item Auditing game.
            \begin{itemize}
                \item One researcher modifies network, so that it behaves weirdly.
                \item Another has to figure out what has been done, using only interpretability tools (not looking at error cases).
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item Deliberate design.
    \begin{itemize}
        \item Might lead to more deliberate design of systems.
        \item Currently quite trial and error: throw different techniques at benchmarks and see what sticks.
        \item Learning a little more about how our models work might make risks/lack of complete knowledge more glaring.
    \end{itemize}
    \item Giving feedback on process.
    \begin{itemize}
        \item Help to train better AIs by giving feedback to model not just on output, but on how it is structured.
        \item Help it make the right decisions for the right reasons.
        \item Help the final model be more transparent.
        \item Could get human feedback on model structure using interpretability tools.
        \begin{itemize}
            \item Danger that now optimising to look good wrt this interface.
        \end{itemize}
        \item Model diffing.
        \begin{itemize}
            \item Way of systematically comparing models from point of view of high-level abstractions/concepts.
            \item Can compare how model is evolving over time.
        \end{itemize}
    \end{itemize}
    \item Building microscopes not agents.
    \begin{itemize}
        \item Seems like agents will be more competitive by default.
        \item Don't have to build agents: with shift in culture could instead try to build `microscopes'.
        \begin{itemize}
            \item System examines world and learns from it, but doesn't take actions.
            \item Use interpretability tools to figure out what system has learned.
            \item Decide which action to take based on this.
        \end{itemize}
        \item Google Translate vs.\@ interface which gives access to all knowledge learned by Google Translate.
        \item Microscopes allow us to understand the data.
        \item Microscopes unlikely to be competitive with more traditional techniques.
        \item But it might still be possible to realign ML community towards them, or to use them temporarily if one AI coalition is significantly more advanced than the rest.
    \end{itemize}
\end{itemize}


\subsection{What if interpretability breaks down as AI gets more powerful?}

\begin{itemize}
    \item Chris: very strong interpretability is possible.
    \item Can take NN and reverse compile it into understandable code.
    \item Intuition: NNs do seem to learn meaningful features.
    \item Chris: interpretability varies with model strength.
    \begin{itemize}
        \item Simple models are easily interpretable.
        \item More complex models harder to understand.
        \begin{itemize}
            \item Chris: because this models don't yet have capacity to express the full concepts they need.
            \item Have to rely on compressed proxies.
        \end{itemize}
        \item More advanced models tend to have cleaner, crisper concepts.
        \item Much past human level, interpretability starts to decrease: crisp concepts but more alien and not understandable.
        \item Author: amplified overseer might be able to counter this last trend.
    \end{itemize}
\end{itemize}


\subsection{Improving the field of machine learning}

\begin{itemize}
    \item Realignment of AI community.
    \item Currently: chasing state-of-the-art results.
    \item Not what most scientific fields look like.
    \item Chris' vision.
    \begin{itemize}
        \item Focus on deliberate design.
        \item Progress is made not by trail and error, but by deeply understanding current models.
        \item Large proportion of field dedicated to understanding what models do, and not building new ones.
    \end{itemize}
    \item Two paths.
    \begin{enumerate}[label=\arabic*.]
        \item Get current researchers to switch to interpretability/deliberate design/etc.
        \begin{itemize}
            \item Way for researchers without access to vast amounts of compute to stay relevant.
            \item Lots of low hanging fruits which could yield impressive results.
            \item Very aligned with traditional scientific values.
            \item Current unit of academic credit in ML is publishing papers, evaluated based on the state-of-the art on some benchmark.
            \item Chris founded Distill.
            \begin{itemize}
                \item Aim: adapter between traditional evaluation method and new approach.
                \item Publishes papers visualising systems, or increasing clarity of thought.
                \item Can be recognised as legitimate academic journal.
                \item Interactive diagrams.
            \end{itemize}
        \end{itemize}
        \item Produce new researchers interested in these.
        \begin{itemize}
            \item Chris: convert neuroscientists and system biologists.
            \begin{itemize}
                \item High level similarities.
                \item Would be able to get funding.
                \item Better at running experiments.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \item May well speed up capabilities, by Chris thinks the positive will outweigh the negative.
\end{itemize}


\section{MIRI's Approach}

\url{https://intelligence.org/2015/07/27/miris-approach/}

\begin{itemize}
    \item Biggest specialising assumptions behind MIRI's approach.
    \begin{itemize}
        \item Focus on scenario in which superhuman intelligence is software designed by humans.
        \begin{itemize}
            \item As opposed e.g.\@ to brain emulation.
            \item Difficult to get all the way to brain emulation without reverse engineering and using that knowledge in system.
            \item In any case, any highly reliable AI (superintelligent or not) likely to need at least some parts designed for safety from the group up.
        \end{itemize}
        \item Specialise on technical research.
    \end{itemize}
    \item Key question: \textquote{What would we still be unable to solve, even if the challenge were far simpler?}
    \item Use this question to select open questions.
    \item Filter for problems which are:
    \begin{itemize}
        \item Tractable: can we researched mathematically today.
        \item Uncrowded: unlikely to get solved currently.
        \item Critical: could not safely be delegated to machine.
    \end{itemize}
    \item Key claim: question generator for open problems whose solutions would help us design safer, more reliable systems in future.
\end{itemize}


\subsection{Creating a powerful AI system without understanding why it works is dangerous}

\begin{itemize}
    \item Current practice: researchers do not understand why systems work so well.
    \item Shortcomings being addressed over time: people working on transparency, and on ML foundations.
    \item Meanwhile: trail and error produced led to many advances.
    \item When designing superintelligence, want very high degree of confidence in safety \emph{before} we start online testing.
    \item Extensive testing essential, but not sufficient: need solid understanding of optimisation process to make use bad solutions aren't found.
    \item MIRI: develop tools to understand superintelligence before deployment.
    \item NN researchers probably could have gotten quite far without understanding probability theory, but would lack tools required to understand modern algos.
    \item MIRI: similarly large chunks of AI theory missing.
\end{itemize}


\subsection{We could not yet create a beneficial AI system even via brute force.}

\begin{itemize}
    \item Couldn't brute-force a computer the size of Jupiter to maximise amount of diamond.
    \begin{itemize}
        \item Could do this for asking questions about images, or simulated conversations.
        \item Don't even understand the problem well enough yet to brute-force.
    \end{itemize}
    \item \textquote{There are two types of open problem in AI. One is figuring how to solve in practice problems that we know how to solve in principle. The other is figuring out how to solve in principle problems that we don't even know how to brute force yet.}
    \item MIRI: focuses on second task.
    \item Why can't brute-force diamond maximiser?
    \begin{itemize}
        \item To interact productively in complex environment, system needs to model world, make predictions and rank outcomes.
        \item Don't know how to brute-force world-modeller which produces models inspectable world-models, which can be used by other parts of the system.
        \begin{itemize}
            \item System would need to inspect world-model, to see how much carbon it has.
            \item Where in the data structure are all the carbon atoms?
            \item How do we ensure this is robust to changes in representation: e.g.\@ atoms vs.\@ protons. (Model discovers quantum mechanics)
            \item Need to build multi-level representations of world.
            \item Brute-force technique which optimises black-box world-modellers doesn't yield something which produces inspectable models.
        \end{itemize}
    \end{itemize}
    \item By simplifying to problem of diamond-maximiser, vs.\@ more realistic goals, we factor out some problems we don't know how to solve, even in principle.
\end{itemize}


\subsection{Figuring out how to solve a problem in principle yields many benefits}

\begin{itemize}
    \item If you don't know how to solve problem completely using unlimited computing power, probably not ready to look for heuristic solutions.
    \item Before we have formal understanding of problem, can't be sure what problem is.
    \begin{itemize}
        \item May fail to notice flaws in our reasoning.
        \item May not bring appropriate tools to bear.
    \end{itemize}
    \item Point: identify basic methods useful for solving problem.
    \item By seeking problems which currently can't even be solved in principle, aim to develop tools which programmers of the future will need to build safe, reliable AI systems.
\end{itemize}


\subsection{This is an approach researchers have used successfully in the past}

\begin{itemize}
    \item Rephrase key question: \textquote{can we reduce the problem of building a beneficial AI to some other, simpler problem?}
    \item Common to do this in theoretical computer science and mathematics.
    \item Humanity has good track record of solving problems in advance of when they are needed.
    \begin{itemize}
        \item Church and Turing for computing.
        \item Kolmogorov fo probability theory.
    \end{itemize}
    \item MIRI questions (e.g.\@ \textquote{how would one ideally handle logical uncertainty?} or \textquote{how would one ideally build multi-level world models of a complex environment?}) exist at similar level of generality to past such questions.
    \item By focusing on parts unable to solve even if problem was easier, hone in on parts where core insights are missing.
\end{itemize}


\section{Embedded Agents, Part 1}

\url{https://intelligence.org/2018/10/29/embedded-agents/}

\begin{itemize}
    \item What does it mean to build a learning agent embedded in the real world?
    \item Alexei: video game playing robot.
    \begin{itemize}
        \item Clear input and output channels.
        \item Alexei capable of holding whole video game in mind.
        \item If Alexei has uncertainty, it is only empirical, e.g.\@ about which game his is playing, not logical, like which inputs yield which outputs.
        \item Alexei must store in his mind every possible he could be playing.
        \item Alexei does not have to think about himself.
        \item Only optimising game, not himself.
        \item Can think of himself as unchanging, indivisible atom.
        \item Alexei cleanly separated from environment he is optimising.
    \end{itemize}
    \item Emmy: playing real life.
    \begin{itemize}
        \item Within environment she is trying to optimise.
        \item Doesn't have functional environment.
        \item Since any decision Emmy takes is part of the environment, it's difficult to formalise `choice' for her.
        \begin{itemize}
            \item Generally: how do we formalise that one part of any system has made a `choice', when one one action actually happens.
        \end{itemize}
        \item Since Emmy is smaller than environment, can't store a complete model of environment.
        \begin{itemize}
            \item So can't carry out full Bayesian reasoning.
        \end{itemize}
        \item Emmy capable of self-improvement, but how can she make sure that she only changes in ways that are helpful, and doesn't deviate from her current goal.
        \item Emmy not atom: made of same pieces as environment.
        \item Emmy faces threats from within: might spin up optimising subroutines which become intelligent.
    \end{itemize}
    \item AIXI framework for understanding Alexei.
    \begin{itemize}
        \item Series of actions, observations and rewards.
        \item Actions function of all previous triples.
        \item Used when there is uncertainty in environment: has distribution over all possible environments, and uses this select action.
        \item While environments computable, AIXI itself is not.
    \end{itemize}
    \item Alexei and AIXI \emph{dualistic}: only interaction between agent-stuff and environment-stuff.
    \begin{itemize}
        \item Agent larger than environment.
        \item No self-referential reasoning: agent-stuff different from environment-stuff.
    \end{itemize}
    \item Can use AIXI to get good understanding of Alexei.
    \item Emmy is more confusing.
    \begin{itemize}
        \item Does not have well-defined i/o.
        \item Smaller than environment.
        \item Able to reason about self and self-improve.
        \item Made of parts similar to environment.
    \end{itemize}
    \item Theory of embedded agency: understanding agents like Emmy.
    \item Decision theory.
    \begin{itemize}
        \item Embedded optimisation.
        \item Since we don't have functional environment, argmax doesn't work.
        \item Major open problems.
        \begin{itemize}
            \item Logical counterfactuals: what would happen under $B$, given that we can prove $A$ actually holds?
            \item Environments including copies of the agent.
            \item Logical updatenessless: combine Wei Dai's updateless decision theory with logical uncertainty.
        \end{itemize}
    \end{itemize}
    \item Embedded world-models.
    \begin{itemize}
        \item World-models which include the agent itself.
        \item Difficult: theoretical guarantees ruined by the fact that the true universe is not in hypothesis space, and that we have to make non-Bayesian updates.
        \item Major open problems.
        \begin{itemize}
            \item Logical uncertainty: combine world of logic with world of probability.
            \item Multi-level modelling: several models of same world at different levels of description.
            \item Ontological crises: realise model or goal specified using different ontology from real world.
        \end{itemize}
    \end{itemize}
    \item Robust delegation.
    \begin{itemize}
        \item Agent creating more intelligent successor to help optimise goals.
        \item From point of view of initial agent: how to create successor which will robustly not use intelligence against you.
        \item From point of view of successor: how to robustly learn or respect goals of something much less intelligent, is manipulable, and has a flawed ontology?
        \item Löbian obstacle: impossible to consistently trust things more intelligent than you.
        \item Major open problems.
        \begin{itemize}
            \item Vingean reflection: how to reason about and trust agents smarter than you.
            \item Value learning: how smarter agent can learn goals of weaker.
            \item Corrigibility: get smarter agent to allow (or even help with) modifications to itself.
        \end{itemize}
    \end{itemize}
    \item Subsystem alignment.
    \begin{itemize}
        \item How one unified agent can prevent subsystems fighting with it or with each other.
        \item Sub-agents may arise accidentally, when performing search over sufficiently complex space.
        \item Major open problem: avoid spinning up adversarial subsystems.
    \end{itemize}
\end{itemize}