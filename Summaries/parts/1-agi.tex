%!TEX root = ../summaries.tex

\chapter{Artificial General Intelligence}

\section{Four Background Claims}

\noindent\url{https://intelligence.org/2015/07/24/four-background-claims/}

\begin{itemize}
    \item Claim 1: Humans have a general problem solving ability.
    \begin{itemize}
        \item General across domains.
        \item Some have argued that we only have disparate, specific modules.
    \end{itemize}
    \item Claim 2: An AI system could be much more intelligent than humans.
    \begin{itemize}
        \item Something special about brains?
        \begin{itemize}
            \item Brains are physical systems; according to the Church-Turing thesis, a compute should be able to replicate.
            \item Even if there is a special human feature, what really matters is general problem-solving ability.
        \end{itemize}
        \item Algorithms for general intelligence to complex to program?
        \begin{itemize}
            \item Evolutionary evidence: general intelligence evolved rapidly in humans.
            \item Relative intelligence of dolphins suggests building blocks already present in mouse-like common ancestor. Simulating mouse brain seems quite plausible.
        \end{itemize}
        \item Humans already at or near peak intelligence?
        \begin{itemize}
            \item Would be surprising if humans were perfected reasoners.
            \item Imagine increasing human processing power.
            \item Real bottleneck being able to receive data from physical experiments? Unlikely: many interesting experiments can be sped up.
        \end{itemize}
    \end{itemize}
    \item Claim 3: Super-intelligent AI systems, if built, will shape the future.
    \begin{itemize}
        \item Intelligent beings shape environment to further their goals.
        \item AI system would not be able to defeat humanity as a whole — environment too competitive?
        \begin{itemize}
            \item Selfish actors only integrate into economy as long as it benefits them.
            \item Historically, more technologically advanced civilisations have dominated less.
            \item A number of social and technological advancements seem possible, but have not yet been developed.
            \item Humans coordinate poorly and slowly.
            \item This suggests potential to gain technological advantage.
        \end{itemize}
    \end{itemize}
    \item Intelligent systems won't be beneficial by default.
    \begin{itemize}
        \item To achieve beneficiality, need to solve many technical challenges.
        \item Humans have become more peaceful as we have become more intelligent — will machines learn to act more in accordance with our values?
        \begin{itemize}
            \item Based on misunderstanding of machine intelligence.
        \end{itemize}
    \end{itemize}
\end{itemize}


\section{AGI Safety From First Principles}

\begin{itemize}
    \item Second species argument
    \begin{itemize}
        \item We will build super-intelligent machines.
        \item They will be autonomous agents perusing large-scale goals.
        \item The goals will be mis-aligned with ours.
        \item The development of these systems will lead us to lose control of our future.
    \end{itemize}
    \item Uses examples from ML. Some arguments carry over to systems developed using other techniques.
\end{itemize}


\subsection{Superintelligence}

\begin{itemize}
    \item Intelligence: ability to do well on a broad range of tasks.
    \item Task-based approach to intelligence: specifically optimised for a range of tasks.
    \begin{itemize}
        \item How we use electricity: need to design specific ways to use it for each task.
        \item Current reinforcement learning.
    \end{itemize}
    \item Generalisation-based approach to intelligence: understand new tasks with little or no specific training, generalising from past experience.
    \begin{itemize}
        \item GPT-2, GPT-3.
        \item Meta-learning.
        \item Human learning.
        \begin{itemize}
            \item Learn many specific skills throughout childhood.
            \item These skills are not the same as the economically useful ones we need in adulthood.
            \item Abstraction is key.
        \end{itemize}
    \end{itemize}
    \item Really, it's a spectrum.
    \item Task-based approach likely to yield better results sooner in areas where we have lots of data.
    \begin{itemize}
        \item E.g. self-driving cars, medicine, law, mathematics.
    \end{itemize}
    \item Generalisation-based approach likely to be needed for other areas.
    \begin{itemize}
        \item  E.g. being a CEO, which required a range of skills and has comparatively little data available.
        \item Likely strategy: train AI on other area where we have lots of data, so that it develops necessary cognitive skills.
    \end{itemize}
    \item Potential obstacle to success of generalisation-based approach: could be that in past specific features of ancestral environment or    brains necessary for development of general intelligence.
    \begin{itemize}
        \item E.g. `social arms race' necessary for development of social intelligence.
        \item Likely that any such feature could be simulated.
    \end{itemize}    
\end{itemize}


\section{More Is Different for AI}

\url{https://bounded-regret.ghost.io/more-is-different-for-ai/}


\subsection{More Is Different for AI}

\begin{itemize}
    \item Two approaches to thinking about AI risks.
    \begin{itemize}
        \item Engineering approach.
        \begin{itemize}
            \item Empirically-driven, drawing experience from current ML research.
            \item Looks at things which are either currently major problems, or minor with the potential to become major.
        \end{itemize}
        \item Philosophy approach.
        \begin{itemize}
            \item Thinks about limits of very advanced systems.
            \item Willing to entertain thought experiments which are currently implausible.
        \end{itemize}
    \end{itemize}
    \item Both agree that misaligned objectives are a problem. Philosophy thinks this is a bigger problem.
    \item Both concerned about out-of-distribution generalisation. Philosophy thinks this is a more temporary problem, and is more concerned about situations where we can't provide data even in principle.
    \item Engineering focuses on tasks where ML systems don't perform well. Philosophy focuses on tasks which have an important abstract property.
    \item Philosophy view is significantly underrated by ML researchers.
    \item Engineering view implies need to consider thought experiments.
    \item Philosophy undervalues role of empirical data.
    \item Neither view is satisfactory.
\end{itemize}


\subsection{Future ML Systems Will Be Qualitatively Different}

\begin{itemize}
    \item More is different: quantitative changes in a field can lead to qualitative differences: \emph{emergence}.
    \begin{itemize}
        \item Uranium: with a lot, get a nuclear reaction.
        \item DNA: smaller molecules can't encode data.
        \item Water: hydrogen bonds.
        \item Traffic.
        \item Specialisation: with large enough community of people, not everyone has to be a farmer.
    \end{itemize}
    \item Can be a sharp transition — \emph{phase transition} — or continuous.
    \item Argue that emergence often occurs in AI, and we can expect it con keep happening.
\end{itemize}


\subsubsection{Emergent Shifts in the History of AI}

\begin{itemize}
    \item Increased storage capacities enabled machine learning.
    \item Better hardware enabled neural networks.
    \begin{itemize}
        \item Machine translation: switched from phrase-based models to neural sequence-to-sequence models to fine-tuning a foundation model.
    \end{itemize}
    \item Larger models enabled few-shot and zero-shot learning.
    \begin{itemize}
        \item GPT-2 and GPT-3: unexpectedly arose without specific training.
    \end{itemize}
    \item Grokking: network's generalisation ability improves qualitatively after training for longer if even train accuracy is already very high.
    \item Many other examples of phase-change in model ability after certain number of training steps.
\end{itemize}


\subsubsection{What This Implies for the Engineering Worldview}

\begin{itemize}
    \item Emergence suggests we should expect new qualitatively behaviours not extrapolated from current trends.
    \item Trend: emergence is becoming more common. 
    \item So engineering view can be self-defeating.
    \item How to orient ourselves?
    \begin{itemize}
        \item Adopting new mindsets, in particular incorporating philosophy worldview.
        \item Future ML systems will have weird failure modes not encountered today.
        \item Empirical findings often generalise surprisingly far.
    \end{itemize}
\end{itemize}


\subsection{Thought Experiments Provide a Third Anchor}

\begin{itemize}
    \item Anchors: reference classes broadly analogous to future AI systems which provide way of predicting.
    \item Current ML anchor.
    \item Human anchor.
    \begin{itemize}
        \item Humans very good a some tasks: mastery of external tools, efficient learning, long-term planning.
        \item Risks over-anthropomorphising future AI systems.
    \end{itemize}
    \item Optimisation anchor: imagine ideal optimisers.
    \begin{itemize}
        \item Thought-experimenty.
        \item Ideal optimiser would correctly predict imitative deception.
        \item Power-seeking is useful for many goals.
        \item Ignores facts about current ML systems, which can lead to underconstrained predictions.
    \end{itemize}
    \item Other thought experiments possible.
    \begin{itemize}
        \item What happens if an agent does most of its learning through in-context learning, instead of gradient descent?
        \begin{itemize}
            \item In-context learning: learning that occurs during a single rollout of the model.
            \item GPT-3 is an example.
            \item Plausible that agents behaviour will be less controlled by ``extrinsic'' shaping, and more using whatever ``intrinsic'' learning is entailed by the in-context learning.
            \item Likely to happen eventually, and probably suddenly, since in-context learning is very fast.
        \end{itemize}
    \end{itemize}
    \item Other anchors.
    \begin{itemize}
        \item Non-human animal behaviour.
        \item Evolution.
        \item The economy.
        \item Complex systems: biological systems, organisations, the economy.
    \end{itemize}
    \item While thought experiments point to big-picture issues, often bad at getting the details right.
    \begin{itemize}
        \item So not very good at proposing solutions.
        \item Setups of modern thought experiments don't map cleanly onto ML ontology.
    \end{itemize}
\end{itemize}


\section{Forecasting transformative AI: the ``biological anchors'' method in a nutshell}

\url{https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/}

\begin{itemize}
    \item Summary of \url{https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines}.
    \item Biological anchors: method of forecasting AI development.
    \item Idea: use cost of training current AI systems to estimate when we will be able to train a system as complex as the human brain.
\end{itemize}


\subsection{Model size and task size}

\begin{itemize}
    \item Bio anchors assumes that we can estimate the cost to train a model based on model size and task size.
    \item Model size.
    \begin{itemize}
        \item Current models around size of insect brain; less than that of mouse; less than 1\% of a human brain.
        \item Bio anchors assumes transformative AI would need a model 10 times the size of the human brain.
    \end{itemize}
    \item Task size.
    \begin{itemize}
        \item How costly it is to do train and error or watch a task (get data).
        \item Most contentious part of the analysis.
        \item Some tasks can be broken into smaller tasks, and it may be sufficient to train an AI just on those.
    \end{itemize}
\end{itemize}


\subsection{Estimating the expense}

\begin{itemize}
    \item Training model 10 times the size of the human brain would cost around a million trillion dollars.
    \item Bio anchor assumes that computing power will continue to get cheaper, and that AI labs will get bigger budgets.
\end{itemize}


\subsection{Aggressive or conservative?}

\begin{itemize}
    \item Estimate too aggressive, since modern neural networks are fundamentally limited in their reasoning ability?
    \begin{itemize}
        \item Unconvinced there is a deep or stable distinction between `pattern recognition' and `true understanding'.
        \item Arguments fail to specify exactly what a `true understanding' would look like, in a way that can be used to make predictions.
        \item For bio anchors' predictions to be too aggressive, the necessary breakthroughs would have to be beyond the reach of AI scientists. Likely that there will be an influx of talent.
    \end{itemize}
    \item Computing power may not be the only bottleneck: we need researchers to set up the system.
    \item Could be too conservative.
    \begin{itemize}
        \item Perhaps we can directly program, or use a combination with trail-and-error, to create transformative AI sooner.
        \item Superior AI techniques which can be trained much faster.
        \item Integration of AI into society could speed up development.
        \item Treatment of task size may be too conservative: most tasks likely to be on smaller end of spectrum.
    \end{itemize}
\end{itemize}


\subsection{Conclusions}

\begin{itemize}
    \item Estimates 10\% chance of transformative AI by 2036, 50\% chance by 2055 and 80\% chance by 2100.
    \item GPT-3 a bit smaller than mouse brains. With 100–1000 times increase, could perform tasks which take human 1 second of thought.
    \item It is only recently that AI systems have gotten this big.
    \item Bio anchors includes analysis of when we can build a computer which can perform all the calculations done by evolution.
    \begin{itemize}
        \item Seems very conservative.
        \item If there are any special features of human brains needed for transformative AI, it's plausible that they could be discovered in this way.
    \end{itemize}
\end{itemize}


\subsection{Pros and cons of the biological anchors method for forecasting transformative AI timelines}

\begin{itemize}
    \item Cons.
    \begin{itemize}
        \item Complex framework.
        \item Relies of multiple assumptions and estimates.
        \begin{itemize}
            \item Whether trail-and-error is enough.
            \item How to compare model sizes with brain sizes.
            \item Characterising task type.
            \item Use of model size and task size to estimate.
            \item Increases in computing power.
            \item Increased investment in AI research.
        \end{itemize}
    \end{itemize}
    \item Pros.
    \begin{itemize}
        \item Relies of objective facts and explicitly stated assumptions.
        \item Fits with intuitions of pace of AI development.
        \item Can compare to development of AI over time and update.
    \end{itemize}
\end{itemize}