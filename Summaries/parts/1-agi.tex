%!TEX root = ../summaries.tex

\chapter{Artificial General Intelligence}

\section{Four Background Claims}

\noindent\url{https://intelligence.org/2015/07/24/four-background-claims/}

\begin{itemize}
    \item Claim 1: Humans have a general problem solving ability.
    \begin{itemize}
        \item General across domains.
        \item Some have argued that we only have disparate, specific modules.
    \end{itemize}
    \item Claim 2: An AI system could be much more intelligent than humans.
    \begin{itemize}
        \item Something special about brains?
        \begin{itemize}
            \item Brains are physical systems; according to the Church-Turing thesis, a compute should be able to replicate.
            \item Even if there is a special human feature, what really matters is general problem-solving ability.
        \end{itemize}
        \item Algorithms for general intelligence to complex to program?
        \begin{itemize}
            \item Evolutionary evidence: general intelligence evolved rapidly in humans.
            \item Relative intelligence of dolphins suggests building blocks already present in mouse-like common ancestor. Simulating mouse brain seems quite plausible.
        \end{itemize}
        \item Humans already at or near peak intelligence?
        \begin{itemize}
            \item Would be surprising if humans were perfected reasoners.
            \item Imagine increasing human processing power.
            \item Real bottleneck being able to receive data from physical experiments? Unlikely: many interesting experiments can be sped up.
        \end{itemize}
    \end{itemize}
    \item Claim 3: Super-intelligent AI systems, if built, will shape the future.
    \begin{itemize}
        \item Intelligent beings shape environment to further their goals.
        \item AI system would not be able to defeat humanity as a whole — environment too competitive?
        \begin{itemize}
            \item Selfish actors only integrate into economy as long as it benefits them.
            \item Historically, more technologically advanced civilisations have dominated less.
            \item A number of social and technological advancements seem possible, but have not yet been developed.
            \item Humans coordinate poorly and slowly.
            \item This suggests potential to gain technological advantage.
        \end{itemize}
    \end{itemize}
    \item Intelligent systems won't be beneficial by default.
    \begin{itemize}
        \item To achieve beneficiality, need to solve many technical challenges.
        \item Humans have become more peaceful as we have become more intelligent — will machines learn to act more in accordance with our values?
        \begin{itemize}
            \item Based on misunderstanding of machine intelligence.
        \end{itemize}
    \end{itemize}
\end{itemize}


\section{AGI Safety From First Principles}

\begin{itemize}
    \item Second species argument
    \begin{itemize}
        \item We will build super-intelligent machines.
        \item They will be autonomous agents perusing large-scale goals.
        \item The goals will be mis-aligned with ours.
        \item The development of these systems will lead us to lose control of our future.
    \end{itemize}
    \item Uses examples from ML. Some arguments carry over to systems developed using other techniques.
\end{itemize}


\subsection{Superintelligence}

\begin{itemize}
    \item Intelligence: ability to do well on a broad range of tasks.
    \item Task-based approach to intelligence: specifically optimised for a range of tasks.
    \begin{itemize}
        \item How we use electricity: need to design specific ways to use it for each task.
        \item Current reinforcement learning.
    \end{itemize}
    \item Generalisation-based approach to intelligence: understand new tasks with little or no specific training, generalising from past experience.
    \begin{itemize}
        \item GPT-2, GPT-3.
        \item Meta-learning.
        \item Human learning.
        \begin{itemize}
            \item Learn many specific skills throughout childhood.
            \item These skills are not the same as the economically useful ones we need in adulthood.
            \item Abstraction is key.
        \end{itemize}
    \end{itemize}
    \item Really, it's a spectrum.
    \item Task-based approach likely to yield better results sooner in areas where we have lots of data.
    \begin{itemize}
        \item E.g. self-driving cars, medicine, law, mathematics.
    \end{itemize}
    \item Generalisation-based approach likely to be needed for other areas.
    \begin{itemize}
        \item  E.g. being a CEO, which required a range of skills and has comparatively little data available.
        \item Likely strategy: train AI on other area where we have lots of data, so that it develops necessary cognitive skills.
    \end{itemize}
    \item Potential obstacle to success of generalisation-based approach: could be that in past specific features of ancestral environment or    brains necessary for development of general intelligence.
    \begin{itemize}
        \item E.g. `social arms race' necessary for development of social intelligence.
        \item Likely that any such feature could be simulated.
    \end{itemize}    
\end{itemize}


\section{More Is Different for AI}

\url{https://bounded-regret.ghost.io/more-is-different-for-ai/}


\subsection{More Is Different for AI}

\begin{itemize}
    \item Two approaches to thinking about AI risks.
    \begin{itemize}
        \item Engineering approach.
        \begin{itemize}
            \item Empirically-driven, drawing experience from current ML research.
            \item Looks at things which are either currently major problems, or minor with the potential to become major.
        \end{itemize}
        \item Philosophy approach.
        \begin{itemize}
            \item Thinks about limits of very advanced systems.
            \item Willing to entertain thought experiments which are currently implausible.
        \end{itemize}
    \end{itemize}
    \item Both agree that misaligned objectives are a problem. Philosophy thinks this is a bigger problem.
    \item Both concerned about out-of-distribution generalisation. Philosophy thinks this is a more temporary problem, and is more concerned about situations where we can't provide data even in principle.
    \item Engineering focuses on tasks where ML systems don't perform well. Philosophy focuses on tasks which have an important abstract property.
    \item Philosophy view is significantly underrated by ML researchers.
    \item Engineering view implies need to consider thought experiments.
    \item Philosophy undervalues role of empirical data.
    \item Neither view is satisfactory.
\end{itemize}


\subsection{Future ML Systems Will Be Qualitatively Different}

\begin{itemize}
    \item More is different: quantitative changes in a field can lead to qualitative differences: \emph{emergence}.
    \begin{itemize}
        \item Uranium: with a lot, get a nuclear reaction.
        \item DNA: smaller molecules can't encode data.
        \item Water: hydrogen bonds.
        \item Traffic.
        \item Specialisation: with large enough community of people, not everyone has to be a farmer.
    \end{itemize}
    \item Can be a sharp transition — \emph{phase transition} — or continuous.
    \item Argue that emergence often occurs in AI, and we can expect it con keep happening.
\end{itemize}


\subsubsection{Emergent Shifts in the History of AI}

\begin{itemize}
    \item Increased storage capacities enabled machine learning.
    \item Better hardware enabled neural networks.
    \begin{itemize}
        \item Machine translation: switched from phrase-based models to neural sequence-to-sequence models to fine-tuning a foundation model.
    \end{itemize}
    \item Larger models enabled few-shot and zero-shot learning.
    \begin{itemize}
        \item GPT-2 and GPT-3: unexpectedly arose without specific training.
    \end{itemize}
    \item Grokking: network's generalisation ability improves qualitatively after training for longer if even train accuracy is already very high.
    \item Many other examples of phase-change in model ability after certain number of training steps.
\end{itemize}


\subsubsection{What This Implies for the Engineering Worldview}

\begin{itemize}
    \item Emergence suggests we should expect new qualitatively behaviours not extrapolated from current trends.
    \item Trend: emergence is becoming more common. 
    \item So engineering view can be self-defeating.
    \item How to orient ourselves?
    \begin{itemize}
        \item Adopting new mindsets, in particular incorporating philosophy worldview.
        \item Future ML systems will have weird failure modes not encountered today.
        \item Empirical findings often generalise surprisingly far.
    \end{itemize}
\end{itemize}


\subsection{Thought Experiments Provide a Third Anchor}

\begin{itemize}
    \item Anchors: reference classes broadly analogous to future AI systems which provide way of predicting.
    \item Current ML anchor.
    \item Human anchor.
    \begin{itemize}
        \item Humans very good a some tasks: mastery of external tools, efficient learning, long-term planning.
        \item Risks over-anthropomorphising future AI systems.
    \end{itemize}
    \item Optimisation anchor: imagine ideal optimisers.
    \begin{itemize}
        \item Thought-experimenty.
        \item Ideal optimiser would correctly predict imitative deception.
        \item Power-seeking is useful for many goals.
        \item Ignores facts about current ML systems, which can lead to underconstrained predictions.
    \end{itemize}
    \item Other thought experiments possible.
    \begin{itemize}
        \item What happens if an agent does most of its learning through in-context learning, instead of gradient descent?
        \begin{itemize}
            \item In-context learning: learning that occurs during a single rollout of the model.
            \item GPT-3 is an example.
            \item Plausible that agents behaviour will be less controlled by ``extrinsic'' shaping, and more using whatever ``intrinsic'' learning is entailed by the in-context learning.
            \item Likely to happen eventually, and probably suddenly, since in-context learning is very fast.
        \end{itemize}
    \end{itemize}
    \item Other anchors.
    \begin{itemize}
        \item Non-human animal behaviour.
        \item Evolution.
        \item The economy.
        \item Complex systems: biological systems, organisations, the economy.
    \end{itemize}
    \item While thought experiments point to big-picture issues, often bad at getting the details right.
    \begin{itemize}
        \item So not very good at proposing solutions.
        \item Setups of modern thought experiments don't map cleanly onto ML ontology.
    \end{itemize}
\end{itemize}