%!TEX root = ../summaries.tex

\chapter{AI Governance, and careers in alignment research}


\section{Careers in alignment}

\url{https://docs.google.com/document/d/1iFszDulgpu1aZcq_aYFG7Nmcr5zgOhaeSwavOMk1akw/edit#heading=h.4whc9v22p7tb}

\begin{itemize}
    \item Part 1: Excerpt from Gleave's Careers in Beneficial AI Research.
    \begin{itemize}
        \item Is Technical AI Research Right for You?
        \begin{itemize}
            \item Technical alignment research one of most high-impact careers for someone with technical background.
            \item Also consider AI strategy and policy.
            \begin{itemize}
                \item Benefit from people with technical background.
            \end{itemize}
            \item Also consider biorisk, global warming, etc.
        \end{itemize}
        \item Skills within Technical AI Research
        \begin{itemize}
            \item Four main skill sets.
            \begin{enumerate}[label=(\Alph*)]
                \item\label{item:se; careers} Software engineering.
                \item\label{item:ml imp; careers} ML implementation.
                \item\label{item:ml rea; careers} ML research direction.
                \item\label{item:tr; careers} Theory research.
            \end{enumerate}
            \item Approximate mapping onto job titles.
            \begin{itemize}
                \item Research engineers: lots of \ref{item:se; careers}, some of \ref{item:ml imp; careers}.
                \item Deep learning researchers: \ref{item:ml imp; careers} and \ref{item:ml rea; careers}.
                \item Other ML researchers: mostly \ref{item:ml rea; careers}, some \ref{item:tr; careers}.
                \item Theory: \ref{item:tr; careers}.
            \end{itemize}
            \item Progress in beneficial AI bottlenecked on researchers and research engineers.
        \end{itemize}
    \end{itemize}
\end{itemize}


\section{AI Governance: Opportunity and Theory of Impact}

\url{https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact}

\begin{itemize}
    \item Primarily aimed at longtermist perspective: long-term risks and opportunities.
    \item Unlike other longtermist work, focus on more conventional AI scenarios.
\end{itemize}


\subsection{Contemporary Policy Challenges}

\begin{itemize}
    \item AI increasingly applied in important domains with societal impact.
    \item Effect seen in \textquote{international tax law, competition/antitrust policy, innovation policy, and national security motivated controls on trade and investment}.
    \item To work in these areas, need to understand both specific policy area and technical AI details.
    \item Important to create community working across these policy areas, are there are interacting phenomena.
\end{itemize}


\subsection{Long-term Risks and Opportunities}

\begin{itemize}
    \item Superintelligence Perspective.
    \begin{itemize}
        \item Superintelligent AI will pose profound risks and opportunities.
        \item Pose threat to human control which dwarfs other natural or anthropogenic risks.
        \item Managing AI competition.
        \begin{itemize}
            \item Problems exasperated if actors perceive themselves to be in an intense winner-take-all race.
            \item Motivated to cut corners.
        \end{itemize}
        \item \emph{Constitution design}.
        \begin{itemize}
            \item How developer should institutionalise control over intelligence and share its bounty.
        \end{itemize}
        \item \emph{Governance and AI safety interact}.
        \begin{itemize}
            \item Sometimes substitutes, sometimes complements.
            \item Insights into global risks could accelerate AI safety work.
            \item AI safety work could enable new policies.
        \end{itemize}
    \end{itemize}
    \item Ecology and GPT Perspectives.
    \begin{itemize}
        \item Superintelligence perspective not only route to risk.
        \item Criticised for making strong assumptions about nature of advanced AI systems.
        \item \emph{AI ecology perspective}
        \begin{itemize}
            \item Diverse global ecology of systems.
            \item Degree of agency of these systems varies.
            \item Superintelligence in narrow domains.
            \item May be more likely than general superintelligence.
            \item May be easier to do safely.
        \end{itemize}
        \item \emph{General purpose technology perspective}.
        \begin{itemize}
            \item Don't need to emphasise agent-like AI or even powerful AI.
            \item Instead focus on ways even mundane AI could transform many parts of society.
            \item Examples.
            \begin{itemize}
                \item Dramatically reduce labour share and increase inequality.
                \item Reduce cost of surveillance and repression.
                \item Make global markets more oligopolistic.
                \item Alter logic of wealth production.
                \item Shift military power.
                \item Undermine nuclear stability.
            \end{itemize}
            \item Most close to that expressed by economists and policy makers.
        \end{itemize}
        \item Three perspectives not mutually exclusive.
        \begin{itemize}
            \item E.g.\@ GPT perspective help shape environment in which superintelligence emerges.
        \end{itemize}
    \end{itemize}
    \item Misuse Risks, Accident Risks, Structural Risks.
    \begin{itemize}
        \item Many analyses of risk (especially superintelligence perspective) adopt lenses of misuse or accident.
        \begin{itemize}
            \item Misuse: person uses AI system in unethical way.
            \item Accident: unanticipated consequences of using system.
            \item Place responsibility with actor who could have prevented risk.
            \item Typically, opportunity for safety intervention causally close to harm.
        \end{itemize}
        \item Ecology and GPT perspectives emphasise broader lens of structural risks.
        \begin{itemize}
            \item No single actor responsible to risks.
            \item Arise due to structural dynamics.
            \item Analogy: climate change risk from invention of combustion engine.
        \end{itemize}
        \item More superintelligence perspective implies more investment in cutting edge of AI and AI safety.
        \begin{itemize}
            \item Focus on groups most likely to develop superintelligence.
            \item Ensure they have best culture, values, etc.
        \end{itemize}
        \item More ecology and GPT perspective implies more need to understand safety and governance broadly.
        \begin{itemize}
            \item Risk distributed over broad range of scenarios.
            \item Risk-reduction opportunities also broadly distributed.
            \item More likely that existing social structures will shape risks.
            \item Greater need for collaboration, across broad collection of actors.
        \end{itemize}
        \item Existential-risk-concerned people often prioritise superintelligence perspective.
        \begin{itemize}
            \item Describes concrete and causally close ways humans to lose all power.
            \item Ecology and GPT perspectives also useful for examining existential risks.
            \item They can also illuminate risk factors.
        \end{itemize}
    \end{itemize}
\end{itemize}


\subsection{Concrete Pathways to Existential Risk}

\begin{itemize}
    \item Concrete ways in which ecology and GPT perspectives lead to existential risk.
    \item Nuclear Instability.
    \begin{itemize}
        \item Risk of nuclear war increased with relatively mundane improvements in \textquote{sensor technology, cyberweapons, and autonomous weapons}.
    \end{itemize}
    \item Power Transitions, Uncertainty, and Turbulence.
    \begin{itemize}
        \item Technology can change key parameters underlying global bargains.
        \item Lead to power transitions.
        \item Shift offence-defence balance, making war more tempting, or amplifying fear of being attacked.
        \item Lead to great turbulence, which can lead to breakdown of social structures.
        \item All of this can increase risk of systemic war, or weaken humanity's ability to work collectively.
    \end{itemize}
    \item Inequality, Labour Displacement, Authoritarianism.
    \begin{itemize}
        \item World becomes more unequal, undemocratic and hostile to human labour.
        \item Processes catalysed by AI.
        \begin{itemize}
            \item Winner-take-all markets.
            \item Displacement of labour.
            \item Authoritarian surveillance and control.
        \end{itemize}
        \item Could lead to robust totalitarianism.
        \begin{itemize}
            \item Lock-in of bad values.
            \item Increase other existential risks from weakening of government.
        \end{itemize}
        \item Epistemic Security.
        \begin{itemize}
            \item Use of AI for mass psychological manipulation in democracies.
            \item Reduce ability for mass deliberation.
            \item Lower chances that advanced democracies shape AI advancement.
        \end{itemize}
        \item Value Erosion through Competition.
        \begin{itemize}
            \item Race to develop AI can lead to cutting corners.
            \item Generalises: trade-offs between human values and performance.
            \begin{itemize}
                \item Non-monopolistic markets.
                \item Privacy.
                \item Relative equality.
            \end{itemize}
            \item Erosion of these values.
            \item Competitive dynamics can lead to lock-ins of bad values.
        \end{itemize}
    \end{itemize}
\end{itemize}


\subsection{Prioritization and Theory of Impact}

\begin{itemize}
    \item Allocation of investments depends on nature of problem.
    \item Author sees value in all perspectives.
    \item Advocates broad portfolio of investments.
    \item Still needs prioritisation: what directions to invest in.
    \item \emph{Asset-decision model of research impact}: provide assets to help people who eventually make impactful decisions.
    \begin{itemize}
        \item Technical solutions.
        \item Strategies.
        \item Shared perception of risks.
        \item More cooperative worldview.
        \item Competent advisors.
        \item Credibility, authority, connections for these advisors.
    \end{itemize}
    \item Different perspectives on which assets more important.
    \begin{itemize}
        \item \emph{Product model of research}.
        \begin{itemize}
            \item Value of funding research finding out about specific, important questions.
            \item Good when problems well-defined.
            \item Widely-held.
            \item Perpetuated by researchers.
            \item Author: this substantially underestimates importance of AI safety and governance research.
        \end{itemize}
        \item \emph{Field building model of research}.
        \begin{itemize}
            \item Majority of value comes from ways funding supports growing and improving field.
            \item Focuses on other assets.
            \begin{itemize}
                \item Getting people with diverse experience.
                \item Improving AI governance researchers' competence in relevant areas.
                \item Giving intellectual authority and prestige to those with thoughtful perspectives on long-term AI.
                \item Growing field.
                \item Boosting junior researchers.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item Attaining outcomes of field building model may be best achieved by focusing on producing good research projects.
    \begin{itemize}
        \item Similarly to why governments fund basic research: \textquote{the byproduct national benefits that it produces, such as nationally available expertise, talent networks, spinoff businesses, educational and career opportunities, and national absorptive capacity for cutting edge science}.
        \item Considering the problem of international control of AI, important to build up a field, so that it's ready when the time comes.
        \item This can be done by considering concrete research questions: even though the results may not be directly useful, the competence/research networks/etc.\@ built up by doing it will be.
    \end{itemize}
    \item Analogy: international control of nuclear weapons.
    \begin{itemize}
        \item Though theorised about in advance, actual implementation depended on so many inpredictable factors.
        \item Important to have body of experts, ready and empowered to engage with current situation.
        \item AI problem similar, but requires even more diverse portfolio, because there is more uncertainty.
    \end{itemize}
\end{itemize}